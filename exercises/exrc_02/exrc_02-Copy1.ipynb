{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebabbbab-c235-4789-a004-14e69d06ec9d",
   "metadata": {},
   "source": [
    "# Data Analytics Spring 2023 &mdash; Exercises 2\n",
    "\n",
    "### Onni Roivas (last modified: Fri Jan 27 at 09:05 - peer review polish)\n",
    "\n",
    "- Five problems + round 1 peer review\n",
    "- Round 1 peer review deadline: **Mon Jan 30 at 14:00**\n",
    "- Problems deadline: **Mon Feb 6 at 14:00**\n",
    "- Theme: data wrangling with **pandas** (please use pandas in each problem)\n",
    "- **Make a copy of the original notebook** (right click & duplicate) and add your answers (new cells) there\n",
    "- Remember: **no autosave** (so keep hitting that ctrl-s / cmd-s button)\n",
    "- Please make both your code and your notebook readable\n",
    "- When you are done, run the handin code cell at the end of this notebook\n",
    "- The original notebook may change after publication, but the\n",
    "  changes should be minor\n",
    "- Keep your originals up to date by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cde89ee5-64f7-4289-8065-d36dfc25c58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring...\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"/usr/bin/bash /home/varpha/data_analytics/bin/config.sh\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722411d-45e4-4faa-bafa-06aa0d4bf31b",
   "metadata": {},
   "source": [
    "## Round 1 peer review\n",
    "\n",
    "**Deadline: Mon Jan 30 at 14:00**\n",
    "\n",
    "In case you handed in your round 01 exercises, your folder **private/exrc_01/peer_review** should contain an anonymous round 01 solutions notebook of another student. In addition, some model solutions for round 01 are to be found in the folder **public/model_solutions**.\n",
    "\n",
    "Write a few paragraphs of text (plain or markdown) into your favourite editor and submit by running the code cell below. Please address the following issues:\n",
    " \n",
    "- Are the solutions okay? Can you understand / run the code?<br/>\n",
    "  (as opposed to some wishful brainless copy-pasting done in a hurry)\n",
    "- What do you think about the solutions?\n",
    "- How many points out of 5 do they deserve as a whole?\n",
    "- How many points would you give to yourself and why?\n",
    "- Any feedback or comments to Harri?\n",
    "\n",
    "Harri will read and grade your reviews as follows:\n",
    "- nonexistent or nearly so = 0p\n",
    "- something written = 1-2p\n",
    "- well written 3p.\n",
    "\n",
    "Please ignore the **review_by_AB0410.txt** file in your peer_review directory (see [this Teams message](https://teams.microsoft.com/l/message/19:wpZLQbHG775XujahAR4ScrnPZGiByh6yuWgewJCHXVQ1@thread.tacv2/1674749772631?tenantId=6e9eaaf0-3ff7-4de9-8cd4-1ffbd45951b9&groupId=ba0bbd60-8a1b-42b8-84ff-6de6a3c45ab6&parentMessageId=1674749772631&teamName=Data%20Analytics%20YAMK%2C%20Spring%202023&channelName=General&createdTime=1674749772631&allowXTenantAccess=false)).\n",
    "\n",
    "Instead, **please run the code cell below**. It asks you to copy-paste your review text to a field that will become visible once you run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6fd62-ea3e-4158-a620-adf172f36093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/varpha/data_analytics/lib\")\n",
    "from handin import submit_peer_review\n",
    "submit_peer_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0309e-f6de-4a75-9a23-9d884e75c503",
   "metadata": {},
   "source": [
    "You may double check your review submission by running (copy-paste, press enter) the following in the terminal:\n",
    "\n",
    "> **cat /home/varpha/data_analytics/private/AB0410/exrc_01/peer_review/review_by_AB0410.txt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe4d8d-c0fc-43f7-b336-3fc95dd0dd76",
   "metadata": {},
   "source": [
    "## Problem 1. Profiles\n",
    "\n",
    "The file **private/exrc_02/data/AB0410_prob01_profiles.csv** contains some user profiles.\n",
    "Read the csv into a pandas DataFrame and reorganize it as follows:\n",
    "\n",
    "a) Separate the name and address columns so that there are separate columns for\n",
    "- first name\n",
    "- last name\n",
    "- street address\n",
    "- state\n",
    "- postal code.\n",
    "\n",
    "Keep also the ssn, username, sex, mail and birthdate columns. Drop all the other columns.\n",
    "\n",
    "b) Print all entries where the last name begins with the letter S, sorted by:\n",
    "- sex (ladies first)\n",
    "- state (alphabetically)\n",
    "- age (youngest first).\n",
    "\n",
    "(Print the entries three times, in three different ways.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459dba44-eb4a-4a61-bd6b-0442234b7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass as gt\n",
    "import pandas as pd\n",
    "\n",
    "user = gt.getuser().upper()\n",
    "df = pd.read_csv(f\"./data/{user}_prob01_profiles.csv\")\n",
    "\n",
    "df[\"first_name\"] = df[\"name\"].str.split().str[0]\n",
    "df[\"first_name\"] = df[\"name\"].str.extract(r\"(?:Mr\\.|Mrs\\.|Dr\\.)?\\s*([A-Z][a-z]+)\") #get rid of these mr. or mrs. prefixes\n",
    "df[\"last_name\"] = df[\"name\"].str.split().str[-1]\n",
    "df.loc[df[\"last_name\"].isin([\"Jr.\"]), \"last_name\"] = df[\"name\"].str.split().str[-2] + \" \" + df[\"name\"].str.split().str[-1] #finally from name to last_name and first_name\n",
    "\n",
    "#split the address column into separate columns\n",
    "df[[\"street_address\", \"state\", \"postal_code\"]] = df[\"address\"].str.extract(r\"(.*?),\\s*([A-Z]{2})\\s*(\\d{5}(?:-\\d{4})?)\", expand=True)\n",
    "\n",
    "#drop some columns\n",
    "df = df.drop(columns = [\"name\", \"address\", \"website\", \"residence\", \"mail\", \"blood_group\", \"job\", \"company\"])\n",
    "\n",
    "#sorting the output\n",
    "df = df.sort_values(by=[\"sex\", \"state\", \"birthdate\"])\n",
    "\n",
    "df #final output missing some states and street addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82629f-43fd-4272-a4a2-97addd6614b6",
   "metadata": {},
   "source": [
    "## Problem 2. Weather (part 1/2)\n",
    "\n",
    "The file **private/exrc_02/data/AB0410_prob02_weather.csv** contains hourly weather observations from Helsinki during one month, downloaded from [fmi.fi](https://en.ilmatieteenlaitos.fi/open-data-manual-fmi-wfs-services) (not recommended).\n",
    "\n",
    "First, please do some data cleaning and reorganizing as you find suitable. Then, please answer the following questions:\n",
    "\n",
    "a) How many percentages of the tmax observations are at most one standard deviation away from the total average of (tmax - tmin)?\n",
    "\n",
    "b) Find the top-5 timestamps for the difference between tmax and tmin. For the found rows, print out the following information: timestamp, max temperature, min temperature and temperature difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a4a6d-ddf5-4783-805b-4bc026e7dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass as gt\n",
    "import pandas as pd\n",
    "\n",
    "user = gt.getuser().upper()\n",
    "df = pd.read_csv(f\"./data/{user}_prob02_weather.csv\")\n",
    "\n",
    "#keep rows where value is tmax and tmin\n",
    "df = df[df['ParameterName'].isin(['tmax', 'tmin'])]\n",
    "df = df.pivot(index='Time', columns='ParameterName', values='ParameterValue') #make them columns\n",
    "df['diff'] = df['tmax'] - df['tmin'] #difference of the two\n",
    "\n",
    "# mean and standard definition\n",
    "mean = df['diff'].mean()\n",
    "std = df['diff'].std()\n",
    "\n",
    "# count the number of values that are at most one standard deviation away from the mean and calculate the % of observations\n",
    "count = ((df['diff'] >= mean - std) & (df['diff'] <= mean + std)).sum()\n",
    "percentage = count / df.shape[0] * 100\n",
    "\n",
    "print(\"a)\\n\\n Percentage of tmax observations at most one standard deviation away from the average: {:.2f}%\".format(percentage))\n",
    "\n",
    "# sort by temperature difference, descending\n",
    "df.sort_values(by='diff', ascending=False, inplace=True)\n",
    "\n",
    "#select the top-5 rows and print required info\n",
    "df = df.head(5)\n",
    "print(\"\\nb)\")\n",
    "for i, row in df.iterrows():\n",
    "    print(\"\\nTimestamp: {}\".format(i))\n",
    "    print(\"Max temperature: {}\".format(row['tmax']))\n",
    "    print(\"Min temperature: {}\".format(row['tmin']))\n",
    "    print(\"Temperature difference: {}\".format(row['diff']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc3925-ea46-4069-a059-1adc344dd929",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 3. Premier League Table\n",
    "The file **private/exrc_02/data/AB0410_prob03_epl.csv** has some English Premier League results, downloaded using [this api](https://github.com/miquel-vv/football_data_api).\n",
    "\n",
    "Using the full data, generate a league table which has the team name as the index and columns as follows (a win gives 3 points, a draw gives 1 point, and a loss gives 0 points):\n",
    "* games played\n",
    "* wins\n",
    "* draws\n",
    "* defeats\n",
    "* goals for - goals against\n",
    "* points\n",
    "\n",
    "\n",
    "Sort it with points (most points win). If points are equal, then sorted by\n",
    "* goal difference (goals for - goals against)\n",
    "* goals for\n",
    "\n",
    "\n",
    "The expected result should look something like this (not the same data though):\n",
    "```\n",
    "                games  wins  draws  defeats   goals  points\n",
    "Man City           38    32      4        2  106-27     100\n",
    "Man United         38    25      6        7   68-28      81\n",
    "Tottenham          38    23      8        7   74-36      77\n",
    "Liverpool          38    21     12        5   84-38      75\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b0c15-b9b7-463f-81b7-4f4f5aecdc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass as gt\n",
    "import pandas as pd\n",
    "\n",
    "user = gt.getuser().upper()\n",
    "df = pd.read_csv(f\"./data/{user}_prob03_epl.csv\")\n",
    "\n",
    "# extract the goals data from the fullTime column\n",
    "df['goals'] = df['fullTime'].apply(lambda x: x.replace(\"{'homeTeam': \", '').replace(\", 'awayTeam': \", '-').replace(\"}\", ''))\n",
    "\n",
    "# calculate the points\n",
    "table['points'] = (table['wins'] * 3) + (table['draws'] * 1)\n",
    "\n",
    "# sorting the league table\n",
    "table.sort_values(by=['points', 'goals'], ascending=[False, False], inplace=True)\n",
    "table.reset_index(inplace=True)\n",
    "table = table[['homeTeam', 'games', 'wins', 'draws', 'defeats', 'goals', 'points']]\n",
    "\n",
    "# INCOMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670cdbf-167d-45b5-b2b3-24e1bc8ae4e6",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 4. Weather (part 2/2)\n",
    "\n",
    "The file **private/exrc_02/data/AB0410_prob04_weather.txt** \n",
    "has some (old) weather data from Jyväskylä 1959-2021, again downloaded from [fmi.fi](https://en.ilmatieteenlaitos.fi/open-data-manual-fmi-wfs-services) (not recommended).\n",
    "\n",
    "Calculate the \"snow sum\" (not a meteorological term) for each winter by adding the snow depths for each day of that winter. Start from winter 1959-60 and end to 2019-20 since 1958-59 and 2020-21 are only partial.\n",
    "\n",
    "Notes:\n",
    "* You need to define \"winter\" by yourself.\n",
    "* FMI uses -1 as snow depth when \"there is absolutely no snow at all\". We don\"t want to reduce snow sum in that case, so replace -1 with 0.\n",
    "* For missing data, assume that the snow depth has been the same as during the previous day. (Fill NaNs with the previous valid value.)\n",
    "\n",
    "Then produce a DataFrame that has the winter as the index (in form \"1959-1960\") and columns:\n",
    "* snow sum\n",
    "* snow sum rank among winters so that largest = 1\n",
    "* count of days that snow depth has been over zero\n",
    "* max snow depth of the winter.\n",
    "\n",
    "\n",
    "The three first and the three last rows should look something like:\n",
    "```\n",
    "           Snow sum  rank  count  max\n",
    "Winter                               \n",
    "1959-1960      5593    18    169   65\n",
    "1960-1961      5082    28    162   60\n",
    "1961-1962      6644    12    156   78\n",
    "\n",
    "...\n",
    "\n",
    "2017-2018      6882     8    161   81\n",
    "2018-2019      4030    42    150   54\n",
    "2019-2020      1432    59    112   30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf511d-2bff-4d3b-8cd3-2231732df527",
   "metadata": {},
   "source": [
    "## Problem 5. Statfi data wrangle.\n",
    "- Here we\"re trying to make some sense out of the data that we downloaded from the statfi service in problem 5 of the first exercises. If you did it successfully, please use your own data. Otherwise you may use Harri\"s adoption data at **public/statfi_adoptions_finnish.json**.\n",
    "- Harri had the keyword *adopt*, and the final table *statfin_adopt_pxt_11lv.px*. You may want to replace them in the url below.\n",
    "- First use [this front end](https://pxdata.stat.fi/PxWeb/pxweb/en/StatFin/StatFin__adopt/statfin_adopt_pxt_11lv.px/) to produce a meaningful table, and then try to produce a similar table with pandas and your data.\n",
    "- The most elegant solution wins!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0e70b-1a6e-40f7-aba7-d12ae5968d8a",
   "metadata": {},
   "source": [
    "## Handin your final answers by running the code cell below.\n",
    "- Save your latest changes first.\n",
    "- Please remove anything that may identify you to your anonymous reviewer.\n",
    "- You may run the code cell as many times as you wish.\n",
    "- Your permission to write the handin file ends at the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e138532-a8ef-4ad6-9538-30756d495dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/varpha/data_analytics/lib\")\n",
    "from handin import handin_exrc_02\n",
    "handin_exrc_02()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
