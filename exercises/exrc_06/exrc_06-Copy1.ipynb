{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebabbbab-c235-4789-a004-14e69d06ec9d",
   "metadata": {},
   "source": [
    "# Data Analytics Spring 2023 &mdash; Exercises 6\n",
    "\n",
    "### Onni Roivas (last modified: Mon Apr 3 at 12:50)\n",
    "\n",
    "- Five problems + round 5 peer review\n",
    "- Round 5 peer review deadline: **Mon Apr 3 at 14:00**\n",
    "- Problems deadline: **Tue Apr 11 at 14:00**\n",
    "- Theme: classification\n",
    "- **Make a copy of the original notebook** (right click & duplicate) and add your answers (new cells) there\n",
    "- There may be no autosave\n",
    "- **ALWAYS quit working by stopping the server [here](https://jupyter.vle.fi/home), then close your browser tabs**\n",
    "- Please make both your code and your notebook readable\n",
    "- When you are done, run the handin code cell at the end of this notebook\n",
    "- The original notebook may change after publication, but the\n",
    "  changes should be minor\n",
    "- Keep your originals up to date by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde89ee5-64f7-4289-8065-d36dfc25c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('/usr/bin/bash /home/varpha/data_analytics/bin/config.sh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722411d-45e4-4faa-bafa-06aa0d4bf31b",
   "metadata": {},
   "source": [
    "## Round 5 peer review\n",
    "\n",
    "**Deadline: Mon Apr 3 at 14:00**. Nothing new here!\n",
    "\n",
    "In case you handed in your round 05 exercises, your folder **private/exrc_05/peer_review** should contain an anonymous round 05 solutions notebook of another student. In addition, some model solutions for round 05 are to be found in the folder **public/model_solutions**.\n",
    "\n",
    "Write a few paragraphs of text (plain or markdown) into your favourite text editor and submit by running the code cell below. Please address the following issues:\n",
    " \n",
    "- Are the solutions okay? Can you understand / run the code?<br/>\n",
    "- What do you think about the solutions?\n",
    "- How many points out of 5 do they deserve as a whole?\n",
    "- How many points would you give to yourself and why?\n",
    "- Any feedback or comments to Harri?\n",
    "\n",
    "Harri will read and grade your reviews as follows:\n",
    "- nonexistent or nearly so = 0p\n",
    "- something written = 1-2p\n",
    "- well written 3p.\n",
    "\n",
    "When done with writing your review, please run the code cell below. It asks you to copy-paste your review text to a field that will become visible once you run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6fd62-ea3e-4158-a620-adf172f36093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/varpha/data_analytics/lib')\n",
    "from handin import submit_peer_review\n",
    "submit_peer_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0309e-f6de-4a75-9a23-9d884e75c503",
   "metadata": {},
   "source": [
    "You may double check your review submission by running the following in the terminal (copy-paste & press enter):\n",
    "\n",
    "> **cat /home/varpha/data_analytics/private/AB0410/exrc_05/peer_review/review_by_AB0410.txt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23999bfc-b6fa-4089-91e2-c95bb41571ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Problem 1. Wines\n",
    "\n",
    "\n",
    "[Here](https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p01_wine.csv) is some data on Portuguese wines. \n",
    "\n",
    "Drop rows with missing values.\n",
    "\n",
    "Use logistic regression to predict the type (white/red) from the other fields.\n",
    "\n",
    "Split train/test set 70/30 %. Print the score and the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3724b2b-1ffc-4b3f-afd6-940f0860b95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9845281072717896\n",
      "Confusion matrix:\n",
      " [[ 458   19]\n",
      " [  11 1451]]\n",
      "\n",
      " Within this matrix, the first row represents white wines and second row red wines.\n",
      "\n",
      " There are 458 True Positives for white wines and 1451 True Positives for red wines.\n",
      " There are 19 False positives for white wines and 11 False Positives for red wines.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "df = pd.read_csv('https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p01_wine.csv')\n",
    "\n",
    "\n",
    "# drop rows with empty values\n",
    "df = df.dropna()\n",
    "\n",
    "X = df.drop(['type'], axis=1)\n",
    "y = df['type']\n",
    "\n",
    "# test size 30% with random seed of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# use liblinear solver here\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Score:', accuracy_score(y_test, y_pred))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('\\n Within this matrix, the first row represents white wines and second row red wines.')\n",
    "print('\\n There are 458 True Positives for white wines and 1451 True Positives for red wines.\\n There are 19 False positives for white wines and 11 False Positives for red wines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b19b7cb-bc2e-45b5-963d-0164e6cd1dab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem 2. Voices\n",
    "[Here](https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p02_voice.csv) is some data on human voices ([column info](https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p02_voice.txt)).\n",
    " \n",
    "Predict the label from the other fields using a support vector machine.\n",
    "\n",
    "Split train/test set 70/30 %.\n",
    "\n",
    "Print the score and the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5d5ed6-206e-400c-81e0-728fb0900ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9705573080967402\n",
      "Confusion Matrix:\n",
      "[[437  15]\n",
      " [ 13 486]]\n",
      "same matrix representation as last exercise\n",
      "\n",
      " There are 437 correctly classified male voices \n",
      " There are 486 correctly classified female voices \n",
      " There are 15 misclassified male voices \n",
      " There are 13 misclassified female voices\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "df = pd.read_csv('https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p02_voice.csv')\n",
    "\n",
    "# test size 30% with random seed of 42\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# scaling data, StandardScaler to make sure that all features have equal importance in the svm model\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# support vector machin model\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# results\n",
    "print('Accuracy Score:', score)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "print('same matrix structure as last exercise')\n",
    "print('\\n There are 437 correctly classified male voices \\n There are 486 correctly classified female voices \\n There are 15 misclassified male voices \\n There are 13 misclassified female voices')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4fb28b-cd5c-415d-a097-7c34ef3d100b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem 3. NBA\n",
    "[Here](https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p03_nba.csv) is some data on NBA basketball players in their first season ([column info](https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p03_nba.csv)).\n",
    "\n",
    "Last column tells if player's career has exceed 5 years or not.\n",
    "\n",
    "Fill missing values with the field median.\n",
    "\n",
    "Try to predict if the career has exceeded 5 years or not by using both logistic regression and a support vector machine. Print scores and confusion matrices. Split train/test data as you wish. Compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4292e88-bf3d-4771-bdd3-04c0447931bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6977611940298507\n",
      "Logistic Regression Confusion Matrix:\n",
      " [[ 61  57]\n",
      " [ 24 126]]\n",
      "Support Vector Machine Accuracy: 0.7052238805970149\n",
      "Support Vector Machine Confusion Matrix:\n",
      " [[ 64  54]\n",
      " [ 25 125]]\n",
      "\n",
      "From the first confusion matrix, we can see the following:\n",
      "There are 61 True Negatives which means players whose careers never went past 5 years and were classified correctly\n",
      "There are 126 True Positivies which means players whose careers went past 5 years and were classified correctly\n",
      "There are 57 False Negatives which means which means players whose careers went past 5 years but were incrroctly classified\n",
      "There are 24 False positivies which means players whose careers went past 5 years but were incorrectly classified\n",
      "\n",
      "From the second confusion matrix, we can see the following:\n",
      "There are 64 True Negatives which means players whose careers never went past 5 years and were classified correctly\n",
      "There are 125 True Positivies which means players whose careers went past 5 years and were classified correctly\n",
      "There are 54 False Negatives which means which means players whose careers went past 5 years but were incrroctly classified\n",
      "There are 25 False positivies which means players whose careers went past 5 years but were incorrectly classified\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p03_nba.csv')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Name'] = le.fit_transform(df['Name'])\n",
    "\n",
    "# Replace missing values with median\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='median')\n",
    "df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Preprocessing the data\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Splitting the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0, max_iter=5000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and print scores and confusion matrix for logistic regression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "print('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred_lr))\n",
    "print('Logistic Regression Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "# Support vector machine model\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', random_state=0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print('Support Vector Machine Accuracy:', accuracy_score(y_test, y_pred_svm))\n",
    "print('Support Vector Machine Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_svm))\n",
    "print('\\nFrom the first confusion matrix, we can see the following:')\n",
    "print('There are 61 True Negatives which means players whose careers never went past 5 years and were classified correctly')\n",
    "print('There are 126 True Positivies which means players whose careers went past 5 years and were classified correctly')\n",
    "print('There are 57 False Negatives which means which means players whose careers went past 5 years but were incrroctly classified')\n",
    "print('There are 24 False positivies which means players whose careers went past 5 years but were incorrectly classified')\n",
    "\n",
    "print('\\nFrom the second confusion matrix, we can see the following:')\n",
    "print('There are 64 True Negatives which means players whose careers never went past 5 years and were classified correctly')\n",
    "print('There are 125 True Positivies which means players whose careers went past 5 years and were classified correctly')\n",
    "print('There are 54 False Negatives which means which means players whose careers went past 5 years but were incrroctly classified')\n",
    "print('There are 25 False positivies which means players whose careers went past 5 years but were incorrectly classified')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fd27d-d8a2-4484-a0ec-7e3975f5a231",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem 4.  Mushrooms\n",
    "[Here](https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p04_mushrooms.csv) is some data on mushrooms ([column info](https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p04_mushrooms.txt)).\n",
    "\n",
    "Try to predict the class (edible or poisonous) from the other fields. Use whatever you want!\n",
    "\n",
    "Fields are categorial so one-hot-encoding is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "472b41b9-e5ec-4d85-9071-b1410ad8fabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "XGBoost predicted entire class of all mushrooms correct with 30% test size\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "data = pd.read_csv('https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p04_mushrooms.csv')\n",
    "\n",
    "# encoding the variable as binary (0 for 'e' and 1 for 'p')\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data['class'])\n",
    "\n",
    "# splitting into features and target\n",
    "X = data.drop('class', axis=1)\n",
    "# one-hot-encode\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# EXB technique is taking weak models and creates one single model\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# model the accuracy\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "print('XGBoost predicted entire class of all mushrooms correct with 30% test size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5bf1d-e219-4603-b803-e99e932c2195",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem 5. Loan status\n",
    "[Here](https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p05_loan.txt) is some data on loanees. Last column Loan_Status Y/N, should be predicted from the other field. Use whatever you want.  \n",
    "\n",
    "\n",
    "Do modifications:\n",
    "* Categorial fields to numeric (2-value fields to 0/1, multivalue as dummies)\n",
    "* replace missing values with median\n",
    "* remove rows with outliers: ApplicantIncome, CoapplicantIncome or LoanAmount over 3 standard deviations away from field average\n",
    "\n",
    "\n",
    "Check what would be model's probability to Loan_status = Yes with values:\n",
    "\n",
    "```\n",
    "Gender                   Male\n",
    "Married                    No\n",
    "Dependents                  0\n",
    "Education            Graduate\n",
    "Self_Employed              No\n",
    "ApplicantIncome          2400\n",
    "CoapplicantIncome        2000\n",
    "LoanAmount                 36\n",
    "Loan_Amount_Term          360\n",
    "Credit_History              1\n",
    "Property_Area           Urban\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79c13fa0-66dc-45d1-8946-50982326971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.874439461883408\n",
      "Probability of Loan_Status = Yes: 0.8817631530288667\n",
      "\n",
      "Accuracy means the status of the approval is around 87,44% based on the dataset\n",
      "Probability of Loan_status means that there is 88,18% chance for loan to be approved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = pd.read_csv('https://student.labranet.jamk.fi/~varpha/data_analytics/exrc06p05_loan.txt', delimiter=',')\n",
    "df.dropna(inplace=True) # remove NaNs\n",
    "\n",
    "# categorical features to numeric\n",
    "df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "df['Married'] = df['Married'].map({'Yes': 1, 'No': 0})\n",
    "df['Education'] = df['Education'].map({'Graduate': 1, 'Not Graduate': 0})\n",
    "df['Self_Employed'] = df['Self_Employed'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# dummy variables for Property_Area\n",
    "property_area_dummies = pd.get_dummies(df['Property_Area'], prefix='Property_Area')\n",
    "\n",
    "# new columns to the dataframe\n",
    "df = pd.concat([df, property_area_dummies], axis=1)\n",
    "\n",
    "# Property_Area column\n",
    "df.drop('Property_Area', axis=1, inplace=True)\n",
    "\n",
    "# imputer object with median strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Replace missing values with median\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].median(), inplace=True)\n",
    "df['Credit_History'].fillna(df['Credit_History'].median(), inplace=True)\n",
    "\n",
    "# remove the specified outliers\n",
    "df = df[(np.abs(stats.zscore(df[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']])) < 3).all(axis=1)]\n",
    "\n",
    "# split data into training and testing sets and use test size 30%\n",
    "X = df.drop(columns=['Loan_ID', 'Loan_Status'])\n",
    "y = df['Loan_Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# logistic regression model\n",
    "clf = LogisticRegression(random_state=42, solver='lbfgs', max_iter=1000).fit(X_train, y_train)\n",
    "\n",
    "# predictions on testing set and calculate accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# prediction for input data\n",
    "input_data = {'Gender': 1,\n",
    "              'Married': 0,\n",
    "              'Dependents': 0,\n",
    "              'Education': 1,\n",
    "              'Self_Employed': 0,\n",
    "              'ApplicantIncome': 2400,\n",
    "              'CoapplicantIncome': 2000,\n",
    "              'LoanAmount': 36,\n",
    "              'Loan_Amount_Term': 360,\n",
    "              'Credit_History': 1,\n",
    "              'Property_Area_Rural': 0,\n",
    "              'Property_Area_Semiurban': 0,\n",
    "              'Property_Area_Urban': 1}\n",
    "\n",
    "# input data into dataframe\n",
    "input_df = pd.DataFrame.from_records([input_data])\n",
    "\n",
    "# prediction and get probability of Loan_Status = Yes\n",
    "prob = clf.predict_proba(input_df)[:, 1][0]\n",
    "print(f'Probability of Loan_Status = Yes: {prob}')\n",
    "print('\\nAccuracy means the status of the approval is around 87,44% based on the dataset')\n",
    "print('Probability of Loan_status means that there is 88,18% chance for loan to be approved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0e70b-1a6e-40f7-aba7-d12ae5968d8a",
   "metadata": {},
   "source": [
    "## Handin your final answers by running the code cell below.\n",
    "- Save your latest changes.\n",
    "- Please remove anything that may identify you to your anonymous reviewer. See [this Teams message](https://teams.microsoft.com/l/message/19:wpZLQbHG775XujahAR4ScrnPZGiByh6yuWgewJCHXVQ1@thread.tacv2/1675678597453?tenantId=6e9eaaf0-3ff7-4de9-8cd4-1ffbd45951b9&groupId=ba0bbd60-8a1b-42b8-84ff-6de6a3c45ab6&parentMessageId=1675678597453&teamName=Data%20Analytics%20YAMK%2C%20Spring%202023&channelName=General&createdTime=1675678597453&allowXTenantAccess=false).\n",
    "- You may run the code cell as many times as you wish.\n",
    "- Your permission to write the handin file ends at the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e138532-a8ef-4ad6-9538-30756d495dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the relevant .ipynb files in your current directory:\n",
      "\n",
      "  1. exrc_06-Copy1.ipynb\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input the order number (the one in the beginning of the line) of the file you want to hand in:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your answers to the first exercises were handed in successfully. Thank you!\n",
      "You may double check your handin by calling\n",
      "\n",
      "    /home/varpha/data_analytics/bin/handin.sh\n",
      "\n",
      "from the terminal prompt.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/varpha/data_analytics/lib')\n",
    "from handin import handin_exrc_06\n",
    "handin_exrc_06()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
